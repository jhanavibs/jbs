library(tidyverse)
library(lubridate)
library(ggplot2)
library(readr)
library(datetime)
library(dplyr)
library(forcats)
library(MASS)

#1. Bring this dataset into your R environment.

data<- read.csv("tech_salary_data.csv")
data

#2. Use either the str() function or the glimpse() function from dplyr to learn more about this dataset.

str(data)


#3. Using your assigned seed value, create a data partition.
#Assign approximately 60% of the records to your training set, and the other 40% to your validation set.

set.seed(40)
train.index <- sample(c(1:nrow(data)), nrow(data)*0.6)
train.df <- data[train.index, ]
valid.df <- data[-train.index, ]

View(train.df)

#a. Why is it important to partition the data before doing any sort of in-depth
#analysis of the variables?

#To avoid overfitting

#4.Let’s explore the relationship between yearsofexperience and totalyearlycompensation . Using ggplot, create a scatterplot that depicts
#totalyearlycompensation on the y-axis and yearsofexperience on the x-axis. Add a best-fit
#line to this scatterplot. Use only your training set data to build this plot.

options(scipen=999)
ggplot(train.df, aes(x=yearsofexperience, y=totalyearlycompensation)) + geom_point() +
  geom_smooth(method="lm", se=FALSE)

#5. Now, again using training set data only, find the correlation between yearsofexperience
#and totalyearlycompensation. Then, use cor.test() to see whether this correlation is significant.
#What is this correlation? Is it a strong one? Is the correlation significant?

cor.test(train.df$yearsofexperience,train.df$totalyearlycompensation,use="complete.obs")

#6. Using your training set, create a simple linear regression model, with
#totalyearlycompensation as your outcome variable and yearsofexperience as your input
#variable. Use the summary() function to display the results of your model.

survfit <- lm(totalyearlycompensation~yearsofexperience, data=train.df)
survfit
summary(survfit)

#7. What are the minimum and maximum residual values in this model?

summary(survfit$residuals)

#a. Find the observation whose rating generated the highest residual value in your
#model. What was the person’s actual total compensation? What did the model
#predict that it would be? How is the residual calculated from the two numbers that you just found?

pred = predict(survfit)
max = max(survfit$residuals)
min = min(survfit$residuals)
df = data.frame(train.df$rowNumber, train.df$totalyearlycompensation, train.df$yearsofexperience, survfit$residuals, pred)

filter(df, survfit.residuals==max)


#b. Find the observation whose rating generated the lowest residual value. What was the person’s actual total compensation? What did the model predict that it
#would be? How is the residual calculated from the two numbers that you just found?

filter(df, survfit.residuals==min)


#c. It looks like there are some cases where this model is quite a bit “off the mark.” Write a few sentences with your thoughts about why years of experience does
#not closely predict total compensation.
#tbc

#8. What is the regression equation generated by your model? Make up a hypothetical
#input value and explain what it would predict as an outcome.

#y=1444554.9+10046x

summary(data)

#x=10
#y=1444554.9+10046*10
#y=1545014.9


#9. Using the accuracy() function from the forecast package, assess the accuracy of your
#model against both the training set and the validation set. What is the purpose of
#making this comparison? Focus on RMSE and MAE here in particular

library(forecast)
pred<- predict(survfit,train.df)
accuracy(pred, train.df$totalyearlycompensation)

pred2<- predict(survfit,valid.df)
accuracy(pred, valid.df$totalyearlycompensation)


#10. How does your model’s RMSE compare to the standard deviation of total compensation
#in the training set? What can such a comparison teach us about the model?

RMSE<- sqrt(mean(survfit$residuals^2))
RMSE
sd(train.df$totalyearlycompensation)


##Multiple Linear Regression:

data1<- read.csv("tech_salary_data.csv")
data1

#1. Dataset exploration – missing values.

#a. Are there any missing values in this dataset? If so, which columns have missing
#values?

colSums(is.na(data1))

#b. How could missing values present a problem in linear regression modeling?
#tbc

#c. For any variables with more than 20% missingness, just remove those variables entirely

data1<- data1[,-c(13,14,23,24)]

#d. For any variables with less than 20% missingness, replace the missing values for
#those variables with the most commonly-occurring value for that variable. 

Factor_1<-factor(sample(data1$dmaid,replace=TRUE))
df1<-data.frame(Factor_1)
names(which.max(table(df1$Factor_1)))

Factor_2<-factor(sample(data1$tag,replace=TRUE))
df2<-data.frame(Factor_2)
names(which.max(table(df2$Factor_2)))


data1$dmaid[is.na(data1$dmaid)] = 807
data1$tag[is.na(data1$tag)] = "Full Stack"
data1$level[is.na(data1$level)] = "L5"

colSums(is.na(data1))


#e. Identify one variable that is categorical, and whose values are entirely unique. Remove that variable.
#i. If a categorical variable has entirely unique values/levels, why will it not be useful for predictive purposes? 
                                    

table(data1$timestamp)

data1<-subset (data1, select = -timestamp)

#f. There are three location-related variables in the dataset. What are they?
#location
#cityid
#dmaid

#i. Of the three location-related variables, how many unique levels does each one have? 
#Remove the two that have the highest number of unique levels.

length(unique(data1$location))
length(unique(data1$cityid))
length(unique(data1$dmaid))

data1<-subset (data1, select = -location)
data1<-subset (data1, select = -cityid)


#table(data1$location)
#table(data1$cityid)
#table(data1$dmaid)


#ii. For the remaining location-related variable, perform a filter so that only
#the 8 most frequently-occurring values remain in the dataset 

#data1 %>%  top_n(10,dmaid)

q<-data.frame(table(data1$dmaid))
q1<-q[order(q$Freq,decreasing=TRUE),]
train1<-q1 %>% slice(1:8)

data1<- filter(data1,dmaid %in% c("807","819","0","501","506","635","511","803"))

#g. Next, perform a top_n filtering process to reduce the dataset to rows that contain the top 8 companies only.

#top_n(train2, 8, company) %>% arrange(desc(company))


q2<-data.frame(table(data1$company))
q3<-q2[order(q2$Freq,decreasing=TRUE),]
train2<-q3 %>% slice(1:8)

data1<- filter(data1,company %in% c("Amazon","Microsoft","Google","Facebook","LinkedIn","Cisco","Uber","Salesforce"))

table(data1$company)

#h. Now, perform another top_n filtering process to reduce the dataset to rows that contain the top 8 titles only.

q4<-data.frame(table(data1$title))
q5<-q4[order(q4$Freq,decreasing=TRUE),]
train3<-q5 %>% slice(1:8)

data1<- filter(data1,title %in% c("Software Engineer","Product Manager","Software Engineering Manager","Data Scientist","Technical Program Manager","Product Designer","Hardware Engineer","Marketing"))

table(data1$title)

#i. Now, perform another top_n filtering process to reduce the dataset to rows that contain the top 8 tag values only.

q6<-data.frame(table(data1$tag))
q7<-q6[order(q6$Freq,decreasing=TRUE),]
train4<-q7 %>% slice(1:8)

data1<- filter(data1,tag %in% c("Distributed Systems (Back-End)","Full Stack","API Development (Back-End)","ML / AI","Web Development (Front-End)","Product","Networking","Security"))

table(data1$title)


#j. Remove the level variable and the timestamp variable. We will not use them here.

data1<-subset (data1, select = -level)

#k. Since we will use totalyearlycompensation as our outcome variable, we should remove any variables that are components of totalyearlycompensation. 

data1<-subset (data1, select = -c(basesalary,stockgrantvalue,bonus))


#2. Re-partition the dataset into training and validation sets. Use the same method that
#you used in the SLR portion of the assignment.

set.seed(40)
data1.index <- sample(c(1:nrow(data1)), nrow(data1)*0.6)
train1.df <- data1[data1.index, ]
valid1.df <- data1[-data1.index, ]


#3. Build a correlation table that includes all of the numeric variables in the training set.

names(train1.df)
mynumber<- train1.df[ ,c(3:5,7,8)]
cor(mynumber)
cor(mynumber,use="complete.obs")

#4. Using the backward elimination method shown in the book, build a multiple regression
#model with the data in your training set, with the goal of predicting a consumer’s
#rating. Start with ALL of the remaining predictors in the dataset, regardless of their type.

library(forecast)

train1.df.lm <-lm(totalyearlycompensation ~ ., data = train1.df)
train1.df.lm.step<- step(train1.df.lm, direction="backward")

#a. Use the summary() function in R to demonstrate the MLR model that was
#recommended by the backward elimination process.

summary(train1.df.lm.step)

#5. What is the total sum of squares for your model? (SST).

SSR =  sum((fitted(train1.df.lm.step) - mean(train1.df$totalyearlycompensation))^2)

SSE = sum((fitted(train1.df.lm.step) - train1.df$totalyearlycompensation)^2)

SST= SSR+SSE 

#6. What is the total sum of squares due to regression for your model? (SSR). 

#7. What is your SSR / SST? Where can you also see this value in the summary of your
#regression model?

SSR/SST

#8. Getting from a t-value to a p-value. Choose one of the predictors from your model.
#What is the t-value for that predictor? Using the visualize.t() function from the
#visualize package, create a plot of the t-distribution that shows the distribution for that
#t-value and the number of degrees of freedom in your model. What percent of the
#curve is shaded? How does this relate to the p-value for that predictor?

#companySalesforce<-t-value<- 1.083
#p-value<- 0.27883

library(visualize)

visualize.t(c(-2.302,2.302),df= 9509,section="bounded")

#9. What is your model’s F-Statistic? What does the F-Statistic measure? 

k <- 30
n <- length(train1.df.lm.step$model$yearsofexperience)

numerator <- SSR / k
denominator <- SSE / (n-k-1)
numerator/denominator


#10. Make up a fictional tech worker. For each variable in the model, assign an in-range
#value to your worker. What does your model predict that this person’s total compensation
#will be? To answer this, you can use a function in R or just explain it using the equation
#and some simple math.


#answered in the pdf


#11. Using the accuracy() function from the forecast package, assess the accuracy of your
#model against both the training set and the validation set. What do you notice about these
#results – what do they mean? Describe your findings in a couple of sentences. In this
#section, you should also talk about the way your MLR model differed from your SLR model
#in terms of accuracy.


library(forecast)
preda<- predict(train1.df.lm.step,train1.df)
accuracy(preda, train1.df$totalyearlycompensation)

predb<- predict(train1.df.lm.step,valid1.df)
accuracy(preda, valid1.df$totalyearlycompensation)



